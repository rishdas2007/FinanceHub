Thanks for the log dump—that helped. Here’s what it’s telling us and how to fix it fast.

## What the log actually says

* All the **top-bar** endpoints are healthy and fast (200s, cache hits).
* The scary line is yours: **“CRITICAL: Z-Score performance is severely degraded.”** That fires from your ETF metrics pipeline when either:

  1. You’re computing z-scores on the fly (per request) and it’s slow, **or**
  2. The inputs are degenerate (too few observations or `stdev≈0`), which yields `±∞/NaN`, then the guard throws that “degraded” warning.

The “fabricated / incorrect data” in the **ETF Technical Metrics** section is consistent with #2 (bad inputs → fallbacks) and sometimes with **cache pollution** (mixing new and legacy outputs).

---

## 3 quick checks (run these now)

1. **Do we even have enough history per symbol?**
   You need ≥180–252 daily bars.

   ```sql
   -- Rows + stdev for the last 252 trading days
   SELECT symbol,
          COUNT(*)               AS bars,
          stddev_pop(close)      AS sd_close
   FROM   historical_stock_data
   WHERE  symbol IN ('SPY','XLK','XLV','XLF','XLY','XLI','XLC','XLP','XLE')
     AND  date >= NOW() AT TIME ZONE 'UTC' - INTERVAL '365 days'
   GROUP BY symbol
   ORDER BY symbol;
   ```

   * Any `bars < 180` or `sd_close IS NULL`/`=0` → that symbol should **not** produce z-scores.

2. **Are we dividing by \~zero?**
   In your z-score helper, add an epsilon:

   ```ts
   const EPS = 1e-8;
   export const zScore = (x: number, mean: number, sd: number) =>
     sd > EPS ? (x - mean) / sd : 0;        // and mark row as fallback=true
   ```

3. **Are we recomputing on every request?**
   If yes, latency spikes and the guard trips. You want **precompute + cache**.

---

## Make ETF metrics “boringly correct”

### A) Compute once, cache safely, never fabricate

* **Service layer**

  ```ts
  const MIN_OBS = 180;
  const TTL_MS = 60_000;
  let cache: { t: number; data: any[] } | null = null;

  export async function getAllMetrics(symbols: string[]) {
    const now = Date.now();
    if (cache && now - cache.t < TTL_MS) return cache.data;

    const rows: any[] = [];
    for (const s of symbols) {
      const hist = await loadDailyCloses(s, 400);     // DB → provider fallback
      if (!hist || hist.length < MIN_OBS) {
        rows.push({ symbol: s, fallback: true, reason: 'insufficient_history' });
        continue;
      }
      const closes = hist.map(h => h.close);
      const { mean, sd } = stats(closes.slice(-252)); // Welford one-pass
      const z = zScore(closes.at(-1)!, mean, sd);
      const rsi = computeRSI(closes, 14);
      // … other metrics (ATR, MA gaps) …
      rows.push({ symbol: s, fallback: false, zScore: z, rsi, /* etc. */ });
    }
    cache = { t: now, data: rows };
    return rows;
  }
  ```
* **Controller**: always `{ success:true, data:[…] }` — never `null` — and **no sample/legacy paths**.
* **Client**: if `row.fallback` → badge “No Data” and hide composite; **don’t** fill with dummy numbers.

### B) Invalidate cache across deploys

Include a **pipeline version** in the cache key so stale shapes can’t bleed in:

```ts
const KEY = `etf-metrics:v3`;
```

### C) Guard degenerate inputs

* Bar count < MIN\_OBS → mark fallback
* `sd < EPS` → z=0, mark fallback
* NaN/Infinity anywhere → null out that metric for that row

---

## Stop the “performance degraded” alert

Make the “CRITICAL” guard fire only when **real latency** is high—not on degenerate math:

```ts
const t0 = performance.now();
// compute...
const ms = performance.now() - t0;

if (ms > 250) logger.warn('Z-Score slow', { ms, symbols: symbols.length });
if (hadFallbacks) logger.info('Z-Score partial', { fallbackCount });
```

Right now it likely conflates “slow” with “bad inputs.”

---

## Fix the blank/404 charts you showed

1. **Stock modals (SPY/XLK):** we already switched to UTC daily DB→provider fallback. Ensure you’re returning both:

   ```json
   { "t": 1723248000000, "date": "2025-08-10", "close": 637.18 }
   ```

   and chart uses `XAxis type="number" dataKey="t"`.

2. **Economic modal 404:** add/keep the **compat** route
   `/api/econ/metrics/:id/chart` → map to Silver observations; **never** 404, return empty `data: []` with `warning`.

---

## Clean up the logging noise you pasted

Those `{"type":"Buffer","data":[…]}` lines are your response-body logger printing raw buffers. Decode only JSON and only when small:

```ts
res.on('finish', () => {
  const ct = res.getHeader('content-type');
  if (typeof body === 'object' && String(ct).includes('application/json')) {
    console.log(method, url, status, JSON.stringify(body).slice(0, 2_000));
  }
});
```

It doesn’t cause the bug; it just hides the real signals.

---

## Quick validation checklist (should take 2–3 minutes)

* **Data sufficiency**

  * Run the SQL above; confirm `bars ≥ 180` and `sd_close > 0` for each ETF.
* **API**

  * `GET /api/etf-metrics` → `{ success:true, data:[…] }`

    * No `null`, no uniform dummy values; any missing row has `fallback:true`.
  * `GET /api/stocks/SPY/history?window=30D` → \~30 points with `{t,date,close}`.
  * `GET /api/econ/metrics/10_year_treasury_yield/chart?range=12M` → `{data:[…]}` (not 404).
* **UI**

  * ETF table shows badges (if any fallbacks).
  * Price modals render lines (not blank), axes show real dates.
* **Logs**

  * No more “CRITICAL” message; instead maybe “Z-Score partial” if some symbols lack data.

---

If you can share the output of that SQL (counts + `sd_close`) for the ETF universe, I’ll point at which symbols are tripping the guard and tailor `MIN_OBS`/windowing to your dataset.
